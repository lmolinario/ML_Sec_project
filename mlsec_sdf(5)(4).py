# -*- coding: utf-8 -*-
"""Mlsec_SDF(5)(4).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rAFOhOCpgt5IUX9LYfbnkOooNBkibEym
"""
import os


import robustbench
import secml
import foolbox

print(f"RobustBench version: {robustbench.__spec__}")
print(f"SecML version: {secml.__version__}")
print(f"Foolbox version: {foolbox.__version__}")

from secml.ml.classifiers import CClassifierPyTorch
from robustbench.utils import load_model as robustbench_load_model


model_names = [
    "Standard", #Clean accuracy 94.78%
    "Ding2020MMA",#Clean accuracy 88.02%
    "Rony2019Decoupling",#Clean accuracy 89.05%
    "Rice2020Overfitting",#Clean accuracy 88.67%
    "Wang2023Better_WRN-70-16"#Clean accuracy 95.54%
]


n_samples      = 64
dataset_labels = [
    'airplane', 'automobile', 'bird', 'cat', 'deer',
    'dog', 'frog', 'horse', 'ship', 'truck'
]

input_shape    = (3, 32, 32)


def load_model(model_name):
    model = robustbench_load_model(
        model_name=model_name,
        dataset='cifar10',
        threat_model='L2')

    clf = CClassifierPyTorch(
        model,
        input_shape=input_shape,
        pretrained=True,
        pretrained_classes=CArray(list(range(10))),
        preprocess=None
    )
    return clf

models = []
for name in model_names:
    try:
        model = load_model(name)
        models.append(model)
    except Exception as e:
        print(f"Error loading model {name}: {e}")

from secml.data.loader import CDataLoaderCIFAR10
from secml.ml.features.normalization import CNormalizerMinMax

# Caricare CIFAR-10
tr, ts = CDataLoaderCIFAR10().load()

# Normalizzare
normalizer = CNormalizerMinMax().fit(tr.X)
ts.X = normalizer.transform(ts.X)

# Ridurre a 64 campioni
ts = ts[:64, :]
print(f"Shape delle immagini: {ts.X.shape}")
# Converti CArray in un array NumPy
images_numpy = ts.X.tondarray()

# Riorganizza l'array NumPy in (64, 3, 32, 32)
images_numpy = images_numpy.reshape(-1, 3, 32, 32)

# Verifica la nuova forma
print(f"Nuova shape delle immagini: {images_numpy.shape}")

# Se necessario, puoi anche riconvertire in un oggetto CArray
from secml.array import CArray
ts.X = CArray(images_numpy.reshape(64, 3072))  # Torna al formato (64, 3072) se richiesto

import torch

# Converte le immagini e le etichette in tensori PyTorch
data_tensor = torch.from_numpy(images_numpy).float()
labels_tensor = torch.from_numpy(ts.Y.tondarray()).long()

# Verifica la forma dei tensori
print(f"Tensore immagini: {data_tensor.shape}, Tensore etichette: {labels_tensor.shape}")


# Carica e normalizza i dati di test
test_dataset = CDataLoaderCIFAR10().load()[1]  # Solo i dati di test
test_dataset.X = CNormalizerMinMax().fit_transform(test_dataset.X)

# Riduci a 64 campioni per velocità
test_dataset = test_dataset[:64, :]

# Conversione diretta a PyTorch
data_tensor = torch.tensor(test_dataset.X.tondarray().reshape(-1, 3, 32, 32), dtype=torch.float32)
labels_tensor = torch.tensor(test_dataset.Y.tondarray(), dtype=torch.long)
print(f"Data tensor shape: {data_tensor.shape}, Labels tensor shape: {labels_tensor.shape}")

import os
import sys
if not os.path.exists('/content/SuperDeepFool'):
    print("Directory '/content/SuperDeepFool' non trovata. Verifica il percorso.")
else:
    sys.path.append('/content/SuperDeepFool')
from secml.ml.peval.metrics import CMetricAccuracy

# Calcolo delle predizioni e accuratezza dei modelli
metric = CMetricAccuracy()
models_preds = [clf.predict(ts.X) for clf in models]
accuracies = [metric.performance_score(y_true=ts.Y, y_pred=y_pred) for y_pred in models_preds]

print("-" * 90)
# Stampa delle accuratezze
for idx in range(len(model_names)):
    print(f"Model name: {model_names[idx]:<40} - Clean model accuracy: {(accuracies[idx] * 100):.2f} %")
print("-" * 90)

def attacks_SFD(model, data, labels, model_name):
    """
    Genera campioni avversari utilizzando SuperDeepFool e registra informazioni utili per l'analisi.

    Args:
        model: Il modello da attaccare.
        data: Tensor contenente i dati di input.
        labels: Tensor contenente le etichette vere.
        model_name: Nome del modello attaccato.

    Returns:
        results_dict: Dizionario contenente campioni avversari, perturbazioni e altre informazioni utili.
    """
    import torch
    from SuperDeepFool.superdeepfool.attacks.SuperDeepFool import SuperDeepFool  # Assicurati di avere l'import corretto per SuperDeepFool

    # Identifica il dispositivo (GPU o CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Porta il modello sul dispositivo
    model._model.to(device)

    # Assicurati che i dati siano sul dispositivo corretto
    data = data.to(device, dtype=torch.float32)
    labels = labels.to(device)

    # Inizializza SuperDeepFool
    superdeepfool = SuperDeepFool(
        model=model._model,
        steps=100,
        overshoot=0.02,
        search_iter=10,
        number_of_samples=len(labels),
        l_norm='L2'
    )

    # Genera campioni avversari con SuperDeepFool
    superdeepfool_samples = []
    perturbations = []
    for i in range(data.shape[0]):
        sample = data[i, :].unsqueeze(0)  # Singolo campione
        label = labels[i].unsqueeze(0)    # Singola etichetta
        adversarial = superdeepfool(sample, label)

        # Calcola la perturbazione
        perturbation = adversarial - sample

        superdeepfool_samples.append(adversarial.detach().cpu())
        perturbations.append(perturbation.detach().cpu())

    # Converte i risultati in tensori
    superdeepfool_samples = torch.cat(superdeepfool_samples, dim=0)
    perturbations = torch.cat(perturbations, dim=0)

    # Calcola le predizioni sui campioni originali e avversari
    model._model.eval()  # Imposta il modello in modalità eval
    with torch.no_grad():
        orig_predictions = model._model(data).argmax(dim=1)
        adv_predictions = model._model(superdeepfool_samples.to(device)).argmax(dim=1)

    # Identifica campioni per cui l'attacco ha avuto successo
    successful_attacks = (orig_predictions != adv_predictions).nonzero(as_tuple=True)[0]
    failed_attacks = (orig_predictions == adv_predictions).nonzero(as_tuple=True)[0]

    # Calcola l'accuratezza sui campioni originali e avversari
    orig_accuracy = (orig_predictions == labels).float().mean().item()
    adv_accuracy = (adv_predictions == labels).float().mean().item()

    # Verifica e assegna valori predefiniti se mancano dati
    results_dict = {
        "model_name": model_name,
        "adversarial_samples": superdeepfool_samples if superdeepfool_samples is not None else torch.tensor([]),
        "perturbations": perturbations if perturbations is not None else torch.tensor([]),
        "true_labels": labels.detach().cpu() if labels is not None else torch.tensor([]),
        "orig_predictions": orig_predictions.detach().cpu() if orig_predictions is not None else torch.tensor([]),
        "adv_predictions": adv_predictions.detach().cpu() if adv_predictions is not None else torch.tensor([]),
        "successful_attacks": successful_attacks.detach().cpu() if successful_attacks is not None else torch.tensor([]),
        "failed_attacks": failed_attacks.detach().cpu() if failed_attacks is not None else torch.tensor([]),
        "orig_accuracy": orig_accuracy if orig_accuracy is not None else 0.0,
        "adv_accuracy": adv_accuracy if adv_accuracy is not None else 0.0
    }

    return results_dict

def attacks_FGSM(model, data, labels, model_name, epsilon=0.03):
    """
    Genera campioni avversari utilizzando FGSM e registra informazioni utili per l'analisi.

    Args:
        model: Il modello da attaccare.
        data: Tensor contenente i dati di input.
        labels: Tensor contenente le etichette vere.
        model_name: Nome del modello attaccato.
        epsilon: Intensità della perturbazione FGSM.

    Returns:
        results_dict: Dizionario contenente campioni avversari, perturbazioni e altre informazioni utili.
    """
    import torch
    import torch.nn.functional as F

    # Identifica il dispositivo (GPU o CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Utilizzo del dispositivo: {device}")

    # Porta il modello sul dispositivo
    model._model.to(device)

    # Assicurati che i dati siano sul dispositivo corretto
    data = data.to(device, dtype=torch.float32)
    labels = labels.to(device)

    # Imposta il modello in modalità eval
    model._model.eval()

    # Genera campioni avversari
    fgsm_samples = []
    perturbations = []
    for i in range(data.shape[0]):
        try:
            sample = data[i, :].unsqueeze(0)  # Singolo campione
            label = labels[i].unsqueeze(0)   # Singola etichetta

            # Calcola il gradiente rispetto all'ingresso
            sample.requires_grad = True
            output = model._model(sample)
            loss = F.cross_entropy(output, label)
            model._model.zero_grad()
            loss.backward()

            # Genera l'attacco FGSM
            grad_sign = sample.grad.data.sign()
            adversarial = sample + epsilon * grad_sign
            adversarial = torch.clamp(adversarial, 0, 1)  # Clipping per restare nel range valido

            # Calcola la perturbazione
            perturbation = adversarial - sample

            fgsm_samples.append(adversarial.detach().cpu())
            perturbations.append(perturbation.detach().cpu())
        except Exception as e:
            print(f"Errore nel generare campione avversario per l'indice {i}: {e}")
            continue

    # Converte i risultati in tensori
    fgsm_samples = torch.cat(fgsm_samples, dim=0) if fgsm_samples else torch.tensor([])
    perturbations = torch.cat(perturbations, dim=0) if perturbations else torch.tensor([])

    # Calcola le predizioni sui campioni originali e avversari
    with torch.no_grad():
        orig_predictions = model._model(data).argmax(dim=1)
        adv_predictions = model._model(fgsm_samples.to(device)).argmax(dim=1)

    # Identifica campioni per cui l'attacco ha avuto successo
    successful_attacks = (orig_predictions != adv_predictions).nonzero(as_tuple=True)[0]
    failed_attacks = (orig_predictions == adv_predictions).nonzero(as_tuple=True)[0]

    # Calcola l'accuratezza sui campioni originali e avversari
    orig_accuracy = (orig_predictions == labels).float().mean().item()
    adv_accuracy = (adv_predictions == labels).float().mean().item()

    # Verifica e assegna valori predefiniti se mancano dati
    results_dict = {
        "model_name": model_name,
        "adversarial_samples": fgsm_samples,
        "perturbations": perturbations,
        "true_labels": labels.detach().cpu(),
        "orig_predictions": orig_predictions.detach().cpu(),
        "adv_predictions": adv_predictions.detach().cpu(),
        "successful_attacks": successful_attacks.detach().cpu(),
        "failed_attacks": failed_attacks.detach().cpu(),
        "orig_accuracy": orig_accuracy,
        "adv_accuracy": adv_accuracy
    }

    return results_dict

import os
import pickle

# Controlla se il file 'results_FGSM.pkl' esiste
if os.path.exists('/content/results_FGSM.pkl'):
    print("Il file 'results_FGSM' esiste già. Caricamento dei risultati salvati...")
    try:
        with open('/content/results_FGSM.pkl', 'rb') as f:
            results_FGSM = pickle.load(f)
            if isinstance(results_FGSM, list) and all(isinstance(r, dict) for r in results_FGSM):
                print("Risultati caricati correttamente.")
            else:
                print("Attenzione: il formato dei dati caricati non è quello previsto.")
    except Exception as e:
        print(f"Errore durante il caricamento: {e}")
else:
    # Se il file non esiste, esegui gli attacchi e salva i risultati
    results_FGSM = []
    for idx, model in enumerate(models):
        print(f"Analizzando il modello \"{model_names[idx]}\"...")
        try:
            # Genera i risultati per il modello attuale
            result = attacks_FGSM(model, data_tensor, labels_tensor, model_names[idx])
            results_FGSM.append(result)  # Aggiunge il risultato alla lista
            print(f"Modello: {model_names[idx]} completato.")
        except Exception as e:
            print(f"Errore durante l'analisi del modello {model_names[idx]}: {e}")
            continue

    # Salva i risultati in 'results_FGSM.pkl'
    try:
        with open('/content/results_FGSM.pkl', 'wb') as f:
            pickle.dump(results_FGSM, f)
            print("Risultati salvati nel file 'results_FGSM.pkl'.")
    except Exception as e:
        print(f"Errore durante il salvataggio dei risultati: {e}")

print(results_FGSM)

import os
import pickle

# Controlla se il file 'results.pkl' esiste
if os.path.exists('/content/results_SDF.pkl'):
    print("Il file 'results_SDF' esiste già. Caricamento dei risultati salvati...")
    try:
        with open('/content/results_SDF.pkl', 'rb') as f:
            results_SDF = pickle.load(f)
            if isinstance(results_SDF, list) and all(isinstance(r, dict) for r in results_SDF):
                print("Risultati caricati correttamente.")
            else:
                print("Attenzione: il formato dei dati caricati non è quello previsto.")
    except Exception as e:
        print(f"Errore durante il caricamento: {e}")
else:
    # Se il file non esiste, esegui gli attacchi e salva i risultati
    results_SDF = []
    for idx, model in enumerate(models):
        print(f"Analizzando il modello \"{model_names[idx]}\"...")
        try:
            results= attacks_SFD(model, data_tensor, labels_tensor, model_names[idx])
            results_SDF.append(result)
            print(f"Modello: {model_names[idx]} completato.")
        except Exception as e:
            print(f"Errore durante l'analisi del modello {model_names[idx]}: {e}")
            continue

    # Salva i risultati in 'results_SDF.pkl'
    try:
        with open('results_SDF.pkl', 'wb') as f:
            pickle.dump(results_SDF, f)
            print("Risultati salvati nel file 'results_SDF.pkl'.")
    except Exception as e:
        print(f"Errore durante il salvataggio dei risultati: {e}")

print(results_SDF)

# Itera sugli elementi della lista
for idx, result in enumerate(results_SDF):
    print(f"Risultati per il modello {idx + 1} Attacco SDF:")

    model_name = result.get("model_name", "Modello sconosciuto")
    orig_accuracy = result.get("orig_accuracy", 0.0)
    adv_accuracy = result.get("adv_accuracy", 0.0)
    successful_attacks = result.get("successful_attacks", [])
    failed_attacks = result.get("failed_attacks", [])
    perturbations = result.get("perturbations", torch.tensor([]))

    print(f"Model Name: {model_name}")
    print(f"Original Accuracy: {orig_accuracy * 100:.2f}%")
    print(f"Adversarial Accuracy: {adv_accuracy * 100:.2f}%")

    num_successful = len(successful_attacks)
    num_failed = len(failed_attacks)
    print(f"Numero di attacchi riusciti: {num_successful}")
    print(f"Numero di attacchi falliti: {num_failed}")

    # Calcola la percentuale di successo degli attacchi
    total_attacks = num_successful + num_failed
    if total_attacks > 0:
        success_rate = (num_successful / total_attacks) * 100
        print(f"Percentuale di successo degli attacchi: {success_rate:.2f}%")
    else:
        print("Nessun attacco disponibile per calcolare la percentuale di successo.")

    print("-" * 50)

    # Analizza le perturbazioni generate
    if perturbations.nelement() > 0:
        avg_perturbation_magnitude = torch.mean(torch.norm(perturbations.view(perturbations.size(0), -1), dim=1))
        print(f"Magnitude media delle perturbazioni: {avg_perturbation_magnitude:.4f}")
        print(f"Numero di perturbazioni generate: {perturbations.size(0)}")
    else:
        print("Nessuna perturbazione disponibile nei dati.")

    print("=" * 80)

# Itera sugli elementi della lista
for idx, result in enumerate(results_FGSM):
    print(f"Risultati per il modello {idx + 1} Attacco FGSM:")

    model_name = result.get("model_name", "Modello sconosciuto")
    orig_accuracy = result.get("orig_accuracy", 0.0)
    adv_accuracy = result.get("adv_accuracy", 0.0)
    successful_attacks = result.get("successful_attacks", [])
    failed_attacks = result.get("failed_attacks", [])
    perturbations = result.get("perturbations", torch.tensor([]))

    print(f"Model Name: {model_name}")
    print(f"Original Accuracy: {orig_accuracy * 100:.2f}%")
    print(f"Adversarial Accuracy: {adv_accuracy * 100:.2f}%")

    num_successful = len(successful_attacks)
    num_failed = len(failed_attacks)
    print(f"Numero di attacchi riusciti: {num_successful}")
    print(f"Numero di attacchi falliti: {num_failed}")

    # Calcola la percentuale di successo degli attacchi
    total_attacks = num_successful + num_failed
    if total_attacks > 0:
        success_rate = (num_successful / total_attacks) * 100
        print(f"Percentuale di successo degli attacchi: {success_rate:.2f}%")
    else:
        print("Nessun attacco disponibile per calcolare la percentuale di successo.")

    print("-" * 50)

    # Analizza le perturbazioni generate
    if perturbations.nelement() > 0:
        avg_perturbation_magnitude = torch.mean(torch.norm(perturbations.view(perturbations.size(0), -1), dim=1))
        print(f"Magnitude media delle perturbazioni: {avg_perturbation_magnitude:.4f}")
        print(f"Numero di perturbazioni generate: {perturbations.size(0)}")
    else:
        print("Nessuna perturbazione disponibile nei dati.")

    print("=" * 80)

for idx, result in enumerate(results_SDF):
    failed_indices = result["failed_attacks"]
    orig_preds = result["orig_predictions"][failed_indices]
    adv_preds = result["adv_predictions"][failed_indices]
    print(f"Model {result['model_name']}:")
    print(f"Original predictions: {orig_preds}")
    print(f"Adversarial predictions: {adv_preds}")


for idx, result in enumerate(results_SDF):
    failed_indices = result["failed_attacks"]
    failed_labels = result["true_labels"][failed_indices]
    adv_preds_failed = result["adv_predictions"][failed_indices]
    correct_failed = (failed_labels == adv_preds_failed).sum().item()
    print(f"Model {result['model_name']}:")
    print(f"Correct predictions among failed attacks: {correct_failed}")

for idx, result in enumerate(results_FGSM):
    failed_indices = result["failed_attacks"]
    orig_preds = result["orig_predictions"][failed_indices]
    adv_preds = result["adv_predictions"][failed_indices]
    print(f"Model {result['model_name']}:")
    print(f"Original predictions: {orig_preds}")
    print(f"Adversarial predictions: {adv_preds}")


for idx, result in enumerate(results_FGSM):
    failed_indices = result["failed_attacks"]
    failed_labels = result["true_labels"][failed_indices]
    adv_preds_failed = result["adv_predictions"][failed_indices]
    correct_failed = (failed_labels == adv_preds_failed).sum().item()
    print(f"Model {result['model_name']}:")
    print(f"Correct predictions among failed attacks: {correct_failed}")

import matplotlib.pyplot as plt


# Normalizza l'immagine tra 0 e 1 per la visualizzazione
def normalize_image(img):
    img = (img - img.min()) / (img.max() - img.min())
    return img

# Elenco delle etichette del dataset
dataset_labels = [
    'airplane', 'automobile', 'bird', 'cat', 'deer',
    'dog', 'frog', 'horse', 'ship', 'truck'
]

for result in results_SDF:
    model_name = result["model_name"]
    print(f"\nAnalisi per il modello: {model_name}")

    successful_attacks = result["successful_attacks"]
    failed_attacks = result["failed_attacks"]

    if successful_attacks.nelement() > 0:
        # Immagini originali e avversarie
        successful_orig_images = result["adversarial_samples"][successful_attacks]
        successful_adv_images = result["adversarial_samples"][successful_attacks]
        successful_adv_labels = result["adv_predictions"][successful_attacks]

        # Perturbazioni
        successful_perturbations = result["perturbations"][successful_attacks]
        avg_perturbation_magnitude_successful = torch.mean(torch.norm(successful_perturbations.view(successful_perturbations.size(0), -1), dim=1)).item()
        print(f"Magnitude media perturbazioni attacchi riusciti: {avg_perturbation_magnitude_successful:.4f}")

        # Visualizza immagini e perturbazioni
        num_images = min(5, successful_adv_images.size(0))  # Limitiamo a 5 immagini per modello
        fig, axes = plt.subplots(3, num_images, figsize=(15, 8))

        for i in range(num_images):
            # Etichette come stringhe
            orig_label = dataset_labels[successful_adv_labels[i].item()]
            adv_label = dataset_labels[successful_adv_labels[i].item()]

            # Immagine originale con label
            orig_img = normalize_image(successful_orig_images[i].cpu().numpy())
            if orig_img.ndim == 3:
                axes[0, i].imshow(orig_img.transpose(1, 2, 0))
            else:
                axes[0, i].imshow(orig_img, cmap="gray")
            axes[0, i].set_title(f"Orig: {orig_label}")
            axes[0, i].axis("off")

            # Immagine avversaria con label
            adv_img = normalize_image(successful_adv_images[i].cpu().numpy())
            if adv_img.ndim == 3:
                axes[1, i].imshow(adv_img.transpose(1, 2, 0))
            else:
                axes[1, i].imshow(adv_img, cmap="gray")
            axes[1, i].set_title(f"Adv: {adv_label}")
            axes[1, i].axis("off")

            # Perturbazione
            perturbation = normalize_image(successful_perturbations[i].cpu().numpy())
            if perturbation.ndim == 3:
                axes[2, i].imshow(perturbation.transpose(1, 2, 0))
            else:
                axes[2, i].imshow(perturbation, cmap="gray")
            axes[2, i].set_title("Perturbazione")
            axes[2, i].axis("off")

        plt.tight_layout()
        plt.show()
    else:
        print("Nessun attacco riuscito trovato per il modello.")

import matplotlib.pyplot as plt


# Normalizza l'immagine tra 0 e 1 per la visualizzazione
def normalize_image(img):
    img = (img - img.min()) / (img.max() - img.min())
    return img

# Elenco delle etichette del dataset
dataset_labels = [
    'airplane', 'automobile', 'bird', 'cat', 'deer',
    'dog', 'frog', 'horse', 'ship', 'truck'
]

for result in results_FGSM:
    model_name = result["model_name"]
    print(f"\nAnalisi per il modello: {model_name}")

    successful_attacks = result["successful_attacks"]
    failed_attacks = result["failed_attacks"]

    if successful_attacks.nelement() > 0:
        # Immagini originali e avversarie
        successful_orig_images = result["adversarial_samples"][successful_attacks]
        successful_adv_images = result["adversarial_samples"][successful_attacks]
        successful_adv_labels = result["adv_predictions"][successful_attacks]

        # Perturbazioni
        successful_perturbations = result["perturbations"][successful_attacks]
        avg_perturbation_magnitude_successful = torch.mean(torch.norm(successful_perturbations.view(successful_perturbations.size(0), -1), dim=1)).item()
        print(f"Magnitude media perturbazioni attacchi riusciti: {avg_perturbation_magnitude_successful:.4f}")

        # Visualizza immagini e perturbazioni
        num_images = min(5, successful_adv_images.size(0))  # Limitiamo a 5 immagini per modello
        fig, axes = plt.subplots(3, num_images, figsize=(15, 8))

        for i in range(num_images):
            # Etichette come stringhe
            orig_label = dataset_labels[successful_adv_labels[i].item()]
            adv_label = dataset_labels[successful_adv_labels[i].item()]

            # Immagine originale con label
            orig_img = normalize_image(successful_orig_images[i].cpu().numpy())
            if orig_img.ndim == 3:
                axes[0, i].imshow(orig_img.transpose(1, 2, 0))
            else:
                axes[0, i].imshow(orig_img, cmap="gray")
            axes[0, i].set_title(f"Orig: {orig_label}")
            axes[0, i].axis("off")

            # Immagine avversaria con label
            adv_img = normalize_image(successful_adv_images[i].cpu().numpy())
            if adv_img.ndim == 3:
                axes[1, i].imshow(adv_img.transpose(1, 2, 0))
            else:
                axes[1, i].imshow(adv_img, cmap="gray")
            axes[1, i].set_title(f"Adv: {adv_label}")
            axes[1, i].axis("off")

            # Perturbazione
            perturbation = normalize_image(successful_perturbations[i].cpu().numpy())
            if perturbation.ndim == 3:
                axes[2, i].imshow(perturbation.transpose(1, 2, 0))
            else:
                axes[2, i].imshow(perturbation, cmap="gray")
            axes[2, i].set_title("Perturbazione")
            axes[2, i].axis("off")

        plt.tight_layout()
        plt.show()
    else:
        print("Nessun attacco riuscito trovato per il modello.")

# Normalizza l'immagine tra 0 e 1 per la visualizzazione
def normalize_image(img):
    img = (img - img.min()) / (img.max() - img.min())
    return img

# Elenco delle etichette del dataset
dataset_labels = [
    'airplane', 'automobile', 'bird', 'cat', 'deer',
    'dog', 'frog', 'horse', 'ship', 'truck'
]

for result in results_SDF:
    model_name = result["model_name"]
    print(f"\nAnalisi per il modello: {model_name}")

    successful_attacks = result["successful_attacks"]

    if successful_attacks.nelement() > 0:
        # Estrai le immagini e le predizioni per gli attacchi riusciti
        successful_orig_images = result["adversarial_samples"][successful_attacks]
        successful_adv_images = result["adversarial_samples"][successful_attacks]
        orig_labels = result["true_labels"][successful_attacks]
        adv_labels = result["adv_predictions"][successful_attacks]

        # Filtra i campioni in cui l'attacco è stato efficace
        effective_attacks = orig_labels != adv_labels
        if effective_attacks.sum() == 0:
            print("Nessun attacco efficace trovato per questo modello.")
            continue

        effective_orig_images = successful_orig_images[effective_attacks]
        effective_adv_images = successful_adv_images[effective_attacks]
        effective_orig_labels = orig_labels[effective_attacks]
        effective_adv_labels = adv_labels[effective_attacks]

        # Perturbazioni
        successful_perturbations = result["perturbations"][successful_attacks]
        effective_perturbations = successful_perturbations[effective_attacks]
        avg_perturbation_magnitude_successful = torch.mean(torch.norm(effective_perturbations.view(effective_perturbations.size(0), -1), dim=1)).item()
        print(f"Magnitude media perturbazioni attacchi efficaci: {avg_perturbation_magnitude_successful:.4f}")

        # Visualizza immagini e perturbazioni
        num_images = min(5, effective_adv_images.size(0))  # Limitiamo a 5 immagini per modello
        fig, axes = plt.subplots(3, num_images, figsize=(15, 8))

        for i in range(num_images):
            # Etichette come stringhe
            orig_label_str = dataset_labels[effective_orig_labels[i].item()]
            adv_label_str = dataset_labels[effective_adv_labels[i].item()]

            # Immagine originale con label
            orig_img = normalize_image(effective_orig_images[i].cpu().numpy())
            if orig_img.ndim == 3:
                axes[0, i].imshow(orig_img.transpose(1, 2, 0))
            else:
                axes[0, i].imshow(orig_img, cmap="gray")
            axes[0, i].set_title(f"Orig: {orig_label_str}")
            axes[0, i].axis("off")

            # Immagine avversaria con label
            adv_img = normalize_image(effective_adv_images[i].cpu().numpy())
            if adv_img.ndim == 3:
                axes[1, i].imshow(adv_img.transpose(1, 2, 0))
            else:
                axes[1, i].imshow(adv_img, cmap="gray")
            axes[1, i].set_title(f"Adv: {adv_label_str}")
            axes[1, i].axis("off")

            # Perturbazione
            perturbation = normalize_image(effective_perturbations[i].cpu().numpy())
            if perturbation.ndim == 3:
                axes[2, i].imshow(perturbation.transpose(1, 2, 0))
            else:
                axes[2, i].imshow(perturbation, cmap="gray")
            axes[2, i].set_title("Perturbazione")
            axes[2, i].axis("off")

        plt.tight_layout()
        plt.show()
    else:
        print("Nessun attacco riuscito trovato per il modello.")

# Normalizza l'immagine tra 0 e 1 per la visualizzazione
def normalize_image(img):
    img = (img - img.min()) / (img.max() - img.min())
    return img

# Elenco delle etichette del dataset
dataset_labels = [
    'airplane', 'automobile', 'bird', 'cat', 'deer',
    'dog', 'frog', 'horse', 'ship', 'truck'
]

for result in results_FGSM:
    model_name = result["model_name"]
    print(f"\nAnalisi per il modello: {model_name}")

    successful_attacks = result["successful_attacks"]

    if successful_attacks.nelement() > 0:
        # Estrai le immagini e le predizioni per gli attacchi riusciti
        successful_orig_images = result["adversarial_samples"][successful_attacks]
        successful_adv_images = result["adversarial_samples"][successful_attacks]
        orig_labels = result["true_labels"][successful_attacks]
        adv_labels = result["adv_predictions"][successful_attacks]

        # Filtra i campioni in cui l'attacco è stato efficace
        effective_attacks = orig_labels != adv_labels
        if effective_attacks.sum() == 0:
            print("Nessun attacco efficace trovato per questo modello.")
            continue

        effective_orig_images = successful_orig_images[effective_attacks]
        effective_adv_images = successful_adv_images[effective_attacks]
        effective_orig_labels = orig_labels[effective_attacks]
        effective_adv_labels = adv_labels[effective_attacks]

        # Perturbazioni
        successful_perturbations = result["perturbations"][successful_attacks]
        effective_perturbations = successful_perturbations[effective_attacks]
        avg_perturbation_magnitude_successful = torch.mean(torch.norm(effective_perturbations.view(effective_perturbations.size(0), -1), dim=1)).item()
        print(f"Magnitude media perturbazioni attacchi efficaci: {avg_perturbation_magnitude_successful:.4f}")

        # Visualizza immagini e perturbazioni
        num_images = min(5, effective_adv_images.size(0))  # Limitiamo a 5 immagini per modello
        fig, axes = plt.subplots(3, num_images, figsize=(15, 8))

        for i in range(num_images):
            # Etichette come stringhe
            orig_label_str = dataset_labels[effective_orig_labels[i].item()]
            adv_label_str = dataset_labels[effective_adv_labels[i].item()]

            # Immagine originale con label
            orig_img = normalize_image(effective_orig_images[i].cpu().numpy())
            if orig_img.ndim == 3:
                axes[0, i].imshow(orig_img.transpose(1, 2, 0))
            else:
                axes[0, i].imshow(orig_img, cmap="gray")
            axes[0, i].set_title(f"Orig: {orig_label_str}")
            axes[0, i].axis("off")

            # Immagine avversaria con label
            adv_img = normalize_image(effective_adv_images[i].cpu().numpy())
            if adv_img.ndim == 3:
                axes[1, i].imshow(adv_img.transpose(1, 2, 0))
            else:
                axes[1, i].imshow(adv_img, cmap="gray")
            axes[1, i].set_title(f"Adv: {adv_label_str}")
            axes[1, i].axis("off")

            # Perturbazione
            perturbation = normalize_image(effective_perturbations[i].cpu().numpy())
            if perturbation.ndim == 3:
                axes[2, i].imshow(perturbation.transpose(1, 2, 0))
            else:
                axes[2, i].imshow(perturbation, cmap="gray")
            axes[2, i].set_title("Perturbazione")
            axes[2, i].axis("off")

        plt.tight_layout()
        plt.show()
    else:
        print("Nessun attacco riuscito trovato per il modello.")