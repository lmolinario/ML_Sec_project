# -*- coding: utf-8 -*-
"""MLSEC_FNM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/lmolinario/ML_Sec_project/blob/main/MLSEC_FNM.ipynb

#**Machine Learning Security Project - Project 1**
###**Student: Lello Molinario (70/90/00369)**

**Instructions**

Re-evaluate 5 RobustBench models with another attack algorithm (e.g. FMN) and identify samples for which one attack works and the other doesn't. Explain the results - i.e., provide some motivations on why one of the attacks did not work properly, while the other did.

**Execution**

In order to re-evaluate 5 RobustBench models I chose "FNM" as the attack algorithm.

# **Step 1: Installing libraries and resolving incompatibilities**
"""


import robustbench
import secml
import foolbox as fb


#import secml
#import foolbox as fb
#import robustbench


import os
import gc
import torch
import numpy as np

import matplotlib.pyplot as plt

from secml.array import CArray
from secml.ml import CClassifierPyTorch
from secml.ml.peval.metrics import CMetricAccuracy
from secml.adv.attacks.evasion import CAttackEvasionFoolbox
from secml.data.loader import CDataLoaderCIFAR10
from secml.ml.features.normalization import CNormalizerMinMax
from secml.figure import CFigure
from secml.explanation import CExplainerGradient, CExplainerGradientInput, CExplainerIntegratedGradients
from secml.utils import fm
from secml import settings
from secml.ml.classifiers.loss import CSoftmax

from robustbench.utils import load_model

from torch import nn
import pickle


print(f"RobustBench version: {robustbench.__name__}")
print(f"SecML version: {secml.__version__}")
print(f"Foolbox version: {fb.__version__}")
print(f"Numpy version: {np.__version__}")


"""## Global Variables
Contains definition of global variables
"""

input_shape    = (3, 32, 32)

model_names    = [
    "Ding2020MMA",
    "Wong2020Fast",
    "Andriushchenko2020Understanding",
    "Sitawarin2020Improving",
    "Cui2023Decoupled_WRN-28-10"
]

n_samples      = 64

dataset_labels = [
    'airplane', 'automobile', 'bird', 'cat', 'deer',
    'dog', 'frog', 'horse', 'ship', 'truck'
]

"""## Loading models

Loads five models from robustbench. We have chosen the following models:
*   1 - Ding2020MMA
*   2 - Wong2020Fast
*   3 - Andriushchenko2020Understanding
*   4 - Sitawarin2020Improving
*   5 - Cui2023Decoupled_WRN-28-10

CIFAR-10
Linf, eps=8/255

"""

from secml.ml import CClassifierPyTorch
from secml.array import CArray

def load_model(model_name):
    """
    Carica un modello da RobustBench e lo avvolge in un'istanza di CClassifierPyTorch.
    """
    try:
        # Carica il modello da RobustBench
        model = robustbench.utils.load_model(
            model_name=model_name,
            dataset='cifar10',
            threat_model='Linf',
        )

        # Wrappa il modello in un CClassifierPyTorch
        clf = CClassifierPyTorch(
            model=model,
            input_shape=input_shape,
            pretrained=True,
            pretrained_classes=CArray(range(10)),  # Classi predefinite
            preprocess=None
        )

        print(f"Modello {model_name} avvolto in CClassifierPyTorch correttamente.")
        return clf
    except Exception as e:
        print(f"Errore durante il caricamento del modello {model_name}: {e}")
        return None




models = []
for name in model_names:
    model = load_model(name)
    if model is not None:
        models.append(model)
    else:
        print(f"Errore: il modello {name} non è stato caricato correttamente.")



"""## Loading  CIFAR-10

Loads 64 samples from CIFAR-10 dataset with shape (3, 32, 32)
"""
# Caricare CIFAR-10
tr, ts = CDataLoaderCIFAR10().load()

# Normalizzare i dati
normalizer = CNormalizerMinMax().fit(tr.X)
ts.X = normalizer.transform(ts.X)

# Ridurre a 64 campioni per velocità
ts = ts[:n_samples, :]


# Convertire CArray in NumPy e ristrutturare in (64, 3, 32, 32)
ts.X = CArray(ts.X.tondarray().reshape(-1, 3, 32, 32))





"""## Fast-Minimum-Norm (FMN) attack

Computes the accuracy of the models, just to confirm that it is working properly.
"""

# Calcolo delle predizioni e accuratezza dei modelli
metric        = CMetricAccuracy()
models_preds  = [clf.predict(ts.X) for clf in models]
accuracies    = [metric.performance_score(y_true=ts.Y, y_pred=y_pred) for y_pred in models_preds]

print("-" * 90)
# Stampa delle accuratezze
for idx in range(len(model_names)):
    print(f"Model name: {model_names[idx]:<40} - Clean model accuracy: {(accuracies[idx] * 100):.2f} %")
print("-" * 90)



"""##Runs the FMN attack on each model"""
from secml.adv.attacks.evasion import CAttackEvasionFoolbox
import foolbox as fb


def FNM_attack(samples, labels, model, explainer_class=None, num_classes=10):
    """
    Esegue l'attacco FMN su un singolo modello e raccoglie informazioni di explainability, perturbazione e confidenza.

    :param samples: Campioni (oggetto CArray).
    :param labels: Etichette corrette (oggetto CArray).
    :param model: Modello compatibile con SecML (CClassifier).
    :param explainer_class: Classe dell'explainer (ad es., CExplainerIntegratedGradients).
    :param num_classes: Numero di classi nel dataset.
    :return: Dizionario con i risultati dell'attacco, explainability, perturbazione e confidenza.
    """
    init_params = dict(
        steps=500,
        max_stepsize=1.0,
        gamma=0.05
    )

    if not isinstance(model, CClassifierPyTorch):
        raise ValueError(f"Il modello passato non è un'istanza di CClassifierPyTorch. Tipo: {type(model)}")

    try:
        # Configura l'attacco
        attack = CAttackEvasionFoolbox(
            classifier=model,
            y_target=None,  # Attacco non mirato
            epsilons=8 / 255,
            fb_attack_class=fb.attacks.LInfFMNAttack,
            **init_params
        )

        # Esegui l'attacco
        y_pred, _, adv_ds, _ = attack.run(samples, labels)

        # Calcolo delle spiegazioni (explainability) per i campioni avversari
        attributions = []
        if explainer_class:
            explainer = explainer_class(model)
            for idx in range(adv_ds.X.shape[0]):
                x_adv = adv_ds.X[idx, :]
                attr = CArray.empty(shape=(num_classes, x_adv.size))
                for c in range(num_classes):
                    attr_c = explainer.explain(x_adv, y=c)
                    attr[c, :] = attr_c
                attributions.append(attr)

        # Calcolo della confidenza lungo il percorso di attacco
        x_seq = attack.x_seq
        n_iter = x_seq.shape[0]
        itrs = CArray.arange(n_iter)
        scores = model.predict(x_seq, return_decision_function=True)[1]
        scores = CSoftmax().softmax(scores)

        return {
            'x_seq': x_seq,
            'y_pred_adv': y_pred,
            'adv_ds': adv_ds,
            'attributions': attributions if explainer_class else None,
            'confidence': scores,
            'iterations': itrs
        }

    except Exception as e:
        print(f"Errore durante l'attacco: {e}")
        return {'error': str(e)}




"""##Saves or loads  attack data on the disk"""
for idx, model in enumerate(models):
    if not isinstance(model, CClassifierPyTorch):
        print(f"Errore: Il modello {model_names[idx]} non è un'istanza di CClassifierPyTorch.")
    else:
        print(f"Il modello {model_names[idx]} è caricato correttamente come CClassifierPyTorch.")




results_file = 'results_FNM.pkl'

# Controlla se il file esiste
if os.path.exists(results_file):
    print(f"Il file '{results_file}' esiste già. Caricamento dei risultati salvati...")
    try:
        with open(results_file, 'rb') as f:
            results_FNM = pickle.load(f)
            if isinstance(results_FNM, list) and all(isinstance(r, dict) for r in results_FNM):
                print("Risultati caricati correttamente.")
            else:
                print("Attenzione: il formato dei dati caricati non è quello previsto.")
    except Exception as e:
        print(f"Errore durante il caricamento: {e}")
else:
    # Se il file non esiste, esegui l'attacco
    results_FNM = []
    for idx, model in enumerate(models):
        print(f"Analizzando il modello \"{model_names[idx]}\"...")
        attack_result = FNM_attack(
            samples=ts.X,
            labels=ts.Y,
            model=model,
            explainer_class=CExplainerIntegratedGradients,  # Specifica l'explainer per le spiegazioni
            num_classes=len(dataset_labels)  # CIFAR-10 ha 10 classi
        )
        results_FNM.append({
            'model_name': model_names[idx],
            'result': attack_result
        })
        print(f"Attacco completato sul modello \"{model_names[idx]}\".")

    # Salva i risultati
    try:
        with open(results_file, 'wb') as f:
            pickle.dump(results_FNM, f)
            print(f"Risultati salvati nel file '{results_file}'.")
    except Exception as e:
        print(f"Errore durante il salvataggio dei risultati: {e}")


import pickle
# from google.colab import files

# with open('attack_data.bin', 'wb') as file:
#     pickle.dump(attack_data, file)

# files.download('attack_data.bin')

with open('results_FNM.pkl', 'rb') as file:
    attack_data = pickle.load(file)



# Calcolo delle predizioni e accuratezza dei modelli
metric        = CMetricAccuracy()
models_preds  = [clf.predict(ts.X) for clf in models]
accuracies    = [metric.performance_score(y_true=ts.Y, y_pred=y_pred) for y_pred in models_preds]


print("-" * 90)
# Stampa delle accuratezze dopo l'attacco

for idx in range(len(model_names)):
    accuracy = metric.performance_score(
        y_true=ts.Y,
        y_pred=attack_data[idx]['result']['y_pred_adv']
    )
    print(f"Model name: {model_names[idx]:<40} - Model accuracy under attack: {(accuracy * 100):.2f} %")
print("-" * 90)

####################################################################################################################################


import numpy as np
from secml.figure import CFigure
from secml.array import CArray
from secml.ml.classifiers.loss import CSoftmax

#def convert_image(image):
#    """
#    Converte un'immagine da CArray a NumPy con formato (H, W, C).
#    """
#    return image.tondarray().reshape(input_shape).transpose(1, 2, 0)


def convert_image(image):
    """
    Converte un'immagine da CArray o NumPy in formato (H, W, C).
    """
    if hasattr(image, "tondarray"):  # Se è un CArray
        image = image.tondarray()  # Converti in NumPy
    return image.reshape(input_shape).transpose(1, 2, 0)


def show_image(fig, local_idx, img, img_adv, expl, label, pred):
    """
    Mostra l'immagine originale, avversa, perturbazione e spiegazione.
    """
    fsize=28
    # Calcolo della perturbazione
    diff_img = img_adv - img
    diff_img -= diff_img.min()
    diff_img /= diff_img.max()

    # Calcola la posizione del subplot nella griglia
    fig.subplot(3, 4, local_idx + 1)  # `local_idx` parte da 0

    # Immagine originale
    fig.sp.imshow(convert_image(img))
    fig.sp.title(f"True: {label}", fontsize=fsize)
    fig.sp.xticks([])
    fig.sp.yticks([])

    # Immagine avversa
    fig.subplot(3, 4, local_idx + 2)
    fig.sp.imshow(convert_image(img_adv))
    fig.sp.title(f'Adv: {pred}', fontsize=fsize)
    fig.sp.xticks([])
    fig.sp.yticks([])

    # Perturbazione
    fig.subplot(3, 4, local_idx + 3)
    fig.sp.imshow(convert_image(diff_img))
    fig.sp.title('Perturbation', fontsize=fsize)
    fig.sp.xticks([])
    fig.sp.yticks([])

    # Spiegazione
    expl = convert_image(expl)
    r = np.fabs(expl[:, :, 0])
    g = np.fabs(expl[:, :, 1])
    b = np.fabs(expl[:, :, 2])
    expl = np.maximum(np.maximum(r, g), b)

    fig.subplot(3, 4, local_idx + 4)
    fig.sp.imshow(expl, cmap='seismic')
    fig.sp.title('Explain', fontsize=fsize)
    fig.sp.xticks([])
    fig.sp.yticks([])

print ('y_pred_adv', attack_data)

import numpy as np

# Numero massimo di subplot per figura
max_subplots = 20  # 5 righe x 4 colonne
n_cols = 4  # Numero fisso di colonne
epsilon = 8 / 255  # Limite di perturbazione

# Itera sui modelli
for model_id in range(len(models)):
    print(f"\nVisualizzazione per il modello: {model_names[model_id]}")

    adv_ds = attack_data[model_id]['result']['adv_ds']
    y_adv = attack_data[model_id]['result']['y_pred_adv']
    attributions = attack_data[model_id]['result']['attributions']

    # Reshape delle immagini in formato (n_samples, 3, 32, 32)
    adv_images = adv_ds.X.tondarray().reshape(-1, 3, 32, 32)
    original_images = ts.X.tondarray().reshape(-1, 3, 32, 32)

    # Calcola la distanza L∞ per tutti i campioni
    distances = np.abs(adv_images - original_images).max(axis=(1, 2, 3))

    # Filtra i campioni che soddisfano le condizioni
    selected_indices = [
        idx for idx in range(ts.X.shape[0])
        if (distances[idx] < epsilon and y_adv[idx] != ts.Y[idx])
    ]

    print(f"\nCampioni selezionati per il modello {model_names[model_id]}: {len(selected_indices)}")

    valid_indices = []  # Per salvare i campioni validi
    for idx in selected_indices:
        img = original_images[idx]
        img_adv = adv_images[idx]
        diff_img = img_adv - img

        # Controllo per evitare divisione per zero
        if diff_img.max() > 1e-6:
            valid_indices.append(idx)

        # Interrompe quando abbiamo 3 campioni validi
        if len(valid_indices) == 3:
            break

    print(f"\nCampioni validi per il modello {model_names[model_id]}: {len(valid_indices)}")

    if len(valid_indices) > 0:
        # Crea una nuova figura per i campioni selezionati
        n_rows = len(valid_indices)  # Una riga per ogni campione
        fig = CFigure(height=n_rows * 6, width=18)

        # Aggiungi manualmente il titolo sopra la figura accedendo alla figura Matplotlib interna
        fig.title(f"Explainability for Model: {model_names[model_id]}", fontsize=32)

        for ydx, idx in enumerate(valid_indices):
            img = original_images[idx]
            img_adv = adv_images[idx]
            expl = attributions[idx][y_adv[idx].item(), :]

            # Calcola la differenza e normalizza
            diff_img = img_adv - img
            diff_img /= diff_img.max()  # Sicuro, poiché controllato prima

            # Calcola l'indice locale per il subplot
            local_idx = ydx * 4

            # Mostra l'immagine nel subplot calcolato
            show_image(
                fig,
                local_idx,
                img,
                img_adv,
                expl,
                dataset_labels[ts.Y[idx].item()],
                dataset_labels[y_adv[idx].item()]
            )

        # Completa e salva la figura per i campioni selezionati
        fig.tight_layout(rect=[0, 0.003, 1, 0.94])
        fig.savefig(f"Explainability_model_{model_names[model_id]}.jpg")
        fig.show()

"""
# Visualizzazione del percorso di attacco per ciascun modello
fig = CFigure(width=30, height=4, fontsize=10, linewidth=2)

for model_id in range(len(models)):
    print(f"Predizione in corso per il modello {model_names[model_id]}...")

    if 'x_seq' not in attack_data[model_id]['result']:
        print(f"'x_seq' non trovato per il modello {model_names[model_id]}")
        continue

    x_seq = attack_data[model_id]['result']['x_seq']
    n_iter = x_seq.shape[0]
    itrs = CArray.arange(n_iter)

    # Riduci i dati se necessario
    x_seq = x_seq[::2]  # Prendi 1 punto ogni 2 iterazioni per velocità
    scores = models[model_id].predict(x_seq, return_decision_function=True)[1]

    # Applica il softmax per ottenere valori in [0, 1]
    scores = CSoftmax().softmax(scores)

    fig.subplot(1, 5, model_id + 1)

    if model_id == 0:
        fig.sp.ylabel('confidence')

    fig.sp.xlabel('iteration')

    fig.sp.plot(itrs, scores[:, ts.Y[0]], linestyle='--', c='black')

    if 'y_pred_adv' in attack_data[model_id]:
        fig.sp.plot(itrs, scores[:, attack_data[model_id]['y_pred_adv'][0]], c='black')
    else:
        print(f"'y_pred_adv' non trovato per il modello {model_names[model_id]}")
        continue

    fig.sp.xlim(top=25, bottom=0)

    fig.sp.title(f"Confidence Sample - Model: {model_names[model_id]}")
    fig.sp.legend(['Confidence True Class', 'Confidence Adv. Class'])

fig.tight_layout()
fig.show()
"""