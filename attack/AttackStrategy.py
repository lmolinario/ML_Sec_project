import foolbox as fb  # Foolbox library for adversarial attacks
import torch  # PyTorch framework for deep learning
from autoattack import AutoAttack  # AutoAttack library for adversarial robustness evaluation
from secml.adv.attacks.evasion import CAttackEvasionFoolbox  # Module for adversarial attacks using Foolbox
from secml.array import CArray  # SecML's custom array class
from secml.ml.classifiers.loss import CSoftmax  # Softmax function for classification
from secml.ml.peval.metrics import CMetricAccuracy  # Accuracy metric computation

# Maximum perturbation limit for adversarial attack (Linf norm)
epsilon = 8 / 255  # Converts the value to [0, 1] scale



def compute_explainability(explainer_class, model, adv_ds, num_classes):
    """
    Computes explainability for adversarial samples using an explanation method.

    Parameters:
    - explainer_class: The explainer class to use (e.g., CExplainerIntegratedGradients).
    - model: The model for which explanations are computed.
    - adv_ds: Dataset containing adversarial samples generated by the attack.
    - num_classes: Number of classes in the dataset.

    Returns:
    - attributions (list): A list containing attributions for each adversarial sample.
    """
    attributions = []

    if explainer_class:
        explainer = explainer_class(model)  # Initialize the explainer for the model

        # Iterate over all adversarial samples
        for idx in range(adv_ds.X.shape[0]):
            x_adv = adv_ds.X[idx, :]  # Extract the current adversarial sample

            # Create an empty array to store attributions for each class
            attr = CArray.empty(shape=(num_classes, x_adv.size))

            # Compute attributions for each class
            for c in range(num_classes):
                attr[c, :] = explainer.explain(x_adv, y=c)

            attributions.append(attr)  # Add the attributions to the list

    return attributions



class AttackStrategy:
    """Basic interface for attack strategy."""

    def execute_attack(self, samples, labels, model, model_name, explainer_class=None, num_classes=10):
        raise NotImplementedError("The execute_attack method must be implemented by subclasses.")


class AutoAttackStrategy(AttackStrategy):
    """Implementing AutoAttack as a Strategy."""

    def execute_attack(self, samples, labels, model, model_name, explainer_class=None, num_classes=10):
        """
        Executes AutoAttack on the specified models and collects results,
        including explainability, perturbation, and confidence.

        Parameters:
        - samples (CArray): Input samples to attack.
        - labels (CArray): Corresponding labels for the samples.
        - models (list): List of CClassifierPyTorch models to attack.
        - model_names (list): Names of the corresponding models.
        - explainer_class (optional): Explanation method class (e.g., CExplainerIntegratedGradients).
        - num_classes (int): Number of classes in the dataset.

        Returns:
        - dict: A dictionary with attack results, including adversarial predictions and post-attack accuracy.
        """

        try:
            # Set the device for attack (GPU if available, otherwise CPU)
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            metric = CMetricAccuracy()  # Metric to calculate accuracy

            # Convert samples to PyTorch tensors and move them to GPU/CPU
            x_test_torch = torch.tensor(samples.tondarray(), dtype=torch.float32).to(device)
            y_test_torch = torch.tensor(labels.tondarray(), dtype=torch.long).to(device)

            # Ensure the data has the correct shape [batch, channels, height, width]
            if len(x_test_torch.shape) == 2:
                x_test_torch = x_test_torch.view(-1, *input_shape)

            results = []

            # Iterate over each model to perform AutoAttack
            for idx, model in enumerate(models):
                try:
                    print(f"\nüîç Running AutoAttack on: {model_names[idx]}")

                    # Extract the native PyTorch model from CClassifierPyTorch
                    pytorch_model = model.model.to(device)
                    pytorch_model.eval()  # Set the model to evaluation mode

                    # Create the AutoAttack adversary (Linf norm with epsilon 8/255)
                    adversary = AutoAttack(pytorch_model, norm='Linf', eps=8 / 255)
                    adversary.apgd.n_restarts = 1  # Number of restarts for APGD

                    # Run the attack
                    x_adv_torch = adversary.run_standard_evaluation(x_test_torch, y_test_torch)

                    # Convert adversarial data from PyTorch Tensor to SecML CArray
                    x_adv = CArray(x_adv_torch.cpu().detach().numpy())

                    # Model predictions on adversarial data
                    y_pred_adv = model.predict(x_adv)

                    # Compute accuracy after the attack
                    accuracy_under_attack = metric.performance_score(y_true=labels, y_pred=y_pred_adv)

                    # Compute explainability if requested
                    attributions = compute_explainability(explainer_class, model, x_adv,
                                                          num_classes) if explainer_class else None

                    # Save results for the current model
                    results.append({
                        'model_name': model_names[idx],
                        'x_adv': x_adv,  # Adversarial images
                        'y_pred_adv': y_pred_adv,  # Predictions after the attack
                        'accuracy_under_attack': accuracy_under_attack,  # Model accuracy under attack
                        'attributions': attributions  # Explainability, if available
                    })

                except Exception as e:
                    print(f"Error during attack on {model_names[idx]}: {e}")

            return results  # Returns the list of results for all analyzed models

        except Exception as e:
            print(f"General error during AutoAttack execution: {e}")
            return {'error': str(e)}


class FMNAttackStrategy(AttackStrategy):
    """Implementing FMN Attack as a Strategy."""

    def execute_attack(self, samples, labels, model, model_name, explainer_class=None, num_classes=10):
        """
        Executes the Fast-Minimum-Norm (FMN) attack using Foolbox and collects the results.

        Parameters:
        - samples (CArray): Input samples to attack.
        - labels (CArray): Corresponding labels for the samples.
        - model (CClassifierPyTorch): Target model for the attack.
        - explainer_class (optional): Explanation method class (e.g., CExplainerIntegratedGradients).
        - num_classes (int): Number of classes in the dataset.

        Returns:
        - dict: A dictionary containing the attack results, including adversarial predictions, perturbations, and confidence.
        """

        # FMN attack parameters
        init_params = {
            'steps': 500,  # Maximum number of attack iterations
            'max_stepsize': 1.0,  # Maximum step size for updates
            'gamma': 0.05  # Scaling parameter for finding the minimum perturbation
        }

        try:
            # Configure the FMN attack using Foolbox within SecML
            attack = CAttackEvasionFoolbox(
                classifier=model,  # Model to attack
                y_target=None,  # Untargeted attack (target-free)
                epsilons=epsilon,  # Perturbation limit (Linf norm)
                fb_attack_class=fb.attacks.LInfFMNAttack,  # Foolbox FMN attack class
                **init_params  # Attack parameters
            )

            # Execute the attack and collect results
            y_pred, _, adv_ds, _ = attack.run(samples, labels)

            # Compute explainability if requested
            attributions = compute_explainability(explainer_class, model, adv_ds,
                                                  num_classes) if explainer_class else None

            # Return attack results
            return {
                'x_seq': attack.x_seq,  # Sequence of samples generated during the attack
                'y_pred_adv': y_pred,  # Model predictions after the attack
                'adv_ds': adv_ds,  # Dataset containing adversarial images
                'attributions': attributions,  # Explainability attributions (if requested)
                'confidence': CSoftmax().softmax(model.predict(attack.x_seq, return_decision_function=True)[1]),
                # Class confidence
                'iterations': CArray.arange(attack.x_seq.shape[0])  # Number of attack iterations
            }

        except Exception as e:
            print(f"Error during FMN attack execution: {e}")
            return {'error': str(e)}


class AttackContext:
    """Class that allows you to dynamically select an attack strategy."""

    def __init__(self, strategy: AttackStrategy):
        self.strategy = strategy

    def set_strategy(self, strategy: AttackStrategy):
        """Cambia la strategia di attacco."""
        self.strategy = strategy

    def execute_attack(self, samples, labels, model, model_name, explainer_class=None, num_classes=10):
        """Esegue l'attacco usando la strategia selezionata."""
        return self.strategy.execute_attack(samples, labels, model, model_name, explainer_class, num_classes)

