# -*- coding: utf-8 -*-
"""MLSEC_FNM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/lmolinario/ML_Sec_project/blob/main/MLSEC_FNM.ipynb

#**Machine Learning Security Project - Project 1**
###**Student: Lello Molinario (70/90/00369)**

**Instructions**

Re-evaluate 5 RobustBench models with another attack algorithm (e.g. FMN) and identify samples for which one attack works and the other doesn't. Explain the results - i.e., provide some motivations on why one of the attacks did not work properly, while the other did.

**Execution**

In order to re-evaluate 5 RobustBench models I chose "FNM" as the attack algorithm.

# **Step 1: Installing libraries and resolving incompatibilities**
"""


import robustbench
import secml
import foolbox as fb


#import secml
#import foolbox as fb
#import robustbench


import os
import gc
import torch
import numpy as np

import matplotlib.pyplot as plt

from secml.array import CArray
from secml.ml import CClassifierPyTorch
from secml.ml.peval.metrics import CMetricAccuracy
from secml.adv.attacks.evasion import CAttackEvasionFoolbox
from secml.data.loader import CDataLoaderCIFAR10
from secml.ml.features.normalization import CNormalizerMinMax
from secml.figure import CFigure
from secml.explanation import CExplainerGradient, CExplainerGradientInput, CExplainerIntegratedGradients
from secml.utils import fm
from secml import settings
from secml.ml.classifiers.loss import CSoftmax

from robustbench.utils import load_model

from torch import nn
import pickle


print(f"RobustBench version: {robustbench.__name__}")
print(f"SecML version: {secml.__version__}")
print(f"Foolbox version: {fb.__version__}")
print(f"Numpy version: {np.__version__}")


"""## Global Variables
Contains definition of global variables
"""

input_shape    = (3, 32, 32)

model_names    = [
    "Ding2020MMA",
    "Wong2020Fast",
    "Andriushchenko2020Understanding",
    "Sitawarin2020Improving",
    "Cui2023Decoupled_WRN-28-10"
]

n_samples      = 64

dataset_labels = [
    'airplane', 'automobile', 'bird', 'cat', 'deer',
    'dog', 'frog', 'horse', 'ship', 'truck'
]

"""## Loading models

Loads five models from robustbench. We have chosen the following models:
*   1 - Ding2020MMA
*   2 - Wong2020Fast
*   3 - Andriushchenko2020Understanding
*   4 - Sitawarin2020Improving
*   5 - Cui2023Decoupled_WRN-28-10

CIFAR-10
Linf, eps=8/255

"""

from secml.ml import CClassifierPyTorch
from secml.array import CArray

def load_model(model_name):
    """
    Carica un modello da RobustBench e lo avvolge in un'istanza di CClassifierPyTorch.
    """
    try:
        # Carica il modello da RobustBench
        model = robustbench.utils.load_model(
            model_name=model_name,
            dataset='cifar10',
            threat_model='Linf',
        )

        # Wrappa il modello in un CClassifierPyTorch
        clf = CClassifierPyTorch(
            model=model,
            input_shape=input_shape,
            pretrained=True,
            pretrained_classes=CArray(range(10)),  # Classi predefinite
            preprocess=None
        )

        print(f"Modello {model_name} avvolto in CClassifierPyTorch correttamente.")
        return clf
    except Exception as e:
        print(f"Errore durante il caricamento del modello {model_name}: {e}")
        return None




models = []
for name in model_names:
    model = load_model(name)
    if model is not None:
        models.append(model)
    else:
        print(f"Errore: il modello {name} non è stato caricato correttamente.")



"""## Loading  CIFAR-10

Loads 64 samples from CIFAR-10 dataset with shape (3, 32, 32)
"""
# Caricare CIFAR-10
tr, ts = CDataLoaderCIFAR10().load()

# Normalizzare i dati
normalizer = CNormalizerMinMax().fit(tr.X)
ts.X = normalizer.transform(ts.X)

# Ridurre a 64 campioni per velocità
ts = ts[:n_samples, :]


# Convertire CArray in NumPy e ristrutturare in (64, 3, 32, 32)
ts.X = CArray(ts.X.tondarray().reshape(-1, 3, 32, 32))





"""## Fast-Minimum-Norm (FMN) attack

Computes the accuracy of the models, just to confirm that it is working properly.
"""

# Calcolo delle predizioni e accuratezza dei modelli
metric        = CMetricAccuracy()
models_preds  = [clf.predict(ts.X) for clf in models]
accuracies    = [metric.performance_score(y_true=ts.Y, y_pred=y_pred) for y_pred in models_preds]

print("-" * 90)
# Stampa delle accuratezze
for idx in range(len(model_names)):
    print(f"Model name: {model_names[idx]:<40} - Clean model accuracy: {(accuracies[idx] * 100):.2f} %")
print("-" * 90)



"""##Runs the FMN attack on each model"""
from secml.adv.attacks.evasion import CAttackEvasionFoolbox
import foolbox as fb

def FNM_attack(samples, labels, model):
    """
    Esegue l'attacco FMN su un singolo modello.

    :param samples: Campioni (oggetto CArray).
    :param labels: Etichette corrette (oggetto CArray).
    :param model: Modello compatibile con SecML (CClassifier).
    :return: Dizionario con i risultati dell'attacco.
    """
    init_params = dict(
        steps=500,
        max_stepsize=1.0,
        gamma=0.05
    )
    if not isinstance(model, CClassifierPyTorch):
        raise Valuerror(f"Il modello passato non è un'istanza di CClassifierPyTorch. Tipo: {type(model)}")

    try:
        # Configura l'attacco
        attack = CAttackEvasionFoolbox(
            classifier=model,  # Passa il classificatore
            y_target=None,  # Attacco non mirato
            epsilons=8 / 255,
            fb_attack_class=fb.attacks.LInfFMNAttack,
            **init_params
        )

        # Esegui l'attacco
        y_pred, _, adv_ds, _ = attack.run(samples, labels)

        return {
            'x_seq': attack.x_seq,
            'y_pred_adv': y_pred,
            'adv_ds': adv_ds
        }

    except Exception as e:
        print(f"Errore durante l'attacco: {e}")
        return {'error': str(e)}




"""##Saves or loads  attack data on the disk"""
for idx, model in enumerate(models):
    if not isinstance(model, CClassifierPyTorch):
        print(f"Errore: Il modello {model_names[idx]} non è un'istanza di CClassifierPyTorch.")
    else:
        print(f"Il modello {model_names[idx]} è caricato correttamente come CClassifierPyTorch.")




results_file = '../extracted_data/data_attack_result_FMN.pkl'

# Controlla se il file esiste
if os.path.exists(results_file):
    print(f"Il file '{results_file}' esiste già. Caricamento dei risultati salvati...")
    try:
        with open(results_file, 'rb') as f:
            results_FNM = pickle.load(f)
            if isinstance(results_FNM, list) and all(isinstance(r, dict) for r in results_FNM):
                print("Risultati caricati correttamente.")
            else:
                print("Attenzione: il formato dei dati caricati non è quello previsto.")
    except Exception as e:
        print(f"Errore durante il caricamento: {e}")
else:
    # Se il file non esiste, esegui l'attacco
    results_FNM = []
    for idx, model in enumerate(models):
        print(f"Analizzando il modello \"{model_names[idx]}\"...")
        attack_result = FNM_attack(ts.X, ts.Y, model)
        results_FNM.append({
            'model_name': model_names[idx],
            'result': attack_result
        })
        print(f"Attacco completato sul modello \"{model_names[idx]}\".")

    # Salva i risultati
    try:
        with open(results_file, 'wb') as f:
            pickle.dump(results_FNM, f)
            print(f"Risultati salvati nel file '{results_file}'.")
    except Exception as e:
        print(f"Errore durante il salvataggio dei risultati: {e}")
